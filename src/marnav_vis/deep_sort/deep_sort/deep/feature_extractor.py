import torch
import torchvision.transforms as transforms
import numpy as np
import cv2
import logging
# 作者的原码：
from .model import Net

# 调试时修改为：
# from model import Net

'''
特征提取器：
提取对应bounding box中的特征, 得到一个固定维度的embedding作为该bounding box的代表，
供计算相似度时使用。

模型训练是按照传统ReID的方法进行，使用Extractor类的时候输入为一个list的图片，得到图片对应的特征。
'''

class Extractor(object):
    def __init__(self, model_path, use_cuda=True):
        self.net = Net(reid=True)
        self.device = "cuda" if torch.cuda.is_available() and use_cuda else "cpu"
        state_dict = torch.load(model_path, map_location=lambda storage, loc: storage)['net_dict']
        self.net.load_state_dict(state_dict)
        logger = logging.getLogger("root.tracker")
        logger.info("Loading weights from {}... Done!".format(model_path))
        self.net.to(self.device)
        self.size = (64, 128)
        self.norm = transforms.Compose([
            # RGB图片数据范围是[0-255]，需要先经过ToTensor除以255归一化到[0,1]之后，
            # 再通过Normalize计算(x - mean)/std后，将数据归一化到[-1,1]。
            transforms.ToTensor(),
            # mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]是从imagenet训练集中算出来的
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])
        
    def _preprocess(self, im_crops):
        """
        TODO:
            1. to float with scale from 0 to 1
            2. resize to (64, 128) as Market1501 dataset did
            3. concatenate to a numpy array
            3. to torch Tensor
            4. normalize
        """
        def _resize(im, size):
            return cv2.resize(im.astype(np.float32)/255., size)

        im_batch = torch.cat([self.norm(_resize(im, self.size)).unsqueeze(0) for im in im_crops], dim=0).float()
        return im_batch

# __call__()是一个非常特殊的实例方法。该方法的功能类似于在类中重载 () 运算符，
# 使得类实例对象可以像调用普通函数那样，以“对象名()”的形式使用。
    def __call__(self, im_crops):
        im_batch = self._preprocess(im_crops)
        with torch.no_grad():
            im_batch = im_batch.to(self.device)
            features = self.net(im_batch)
        return features.cpu().numpy()


# 作者的原码：
if __name__ == '__main__':
    img = cv2.imread("ckpt_demo.jpg")[:,:,(2,1,0)]
    extr = Extractor("checkpoint/ckpt.t7")
    feature = extr(img)
    print(feature.shape)

# 调试时为了防止报错，修改为：
# if __name__ == '__main__':
#     img = cv2.imread("ckpt_demo.jpg")  # BGR
#     if img is None:
#         raise ValueError("demo.jpg not found or cannot be read!")
#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 转为RGB
#     if img.ndim == 2:  # 灰度图转3通道
#         img = np.stack([img]*3, axis=-1)
#     extr = Extractor("checkpoint/ckpt.t7")
#     feature = extr([img])  # 注意要用list
#     print(feature.shape)
#     # 输出feature数据前10个元素:
#     print(feature[0][:10])
    
# (1, 512)
# [[0.01852086 0.06843612 0.00567295 0.04675756 0.01879145 0.03756696
#   0.07555637 0.04468011 0.028835   0.0594776  0.03829988 0.04272011
#   0.05258912 0.03620471 0.03715931 0.02676765 0.061597   0.02499721
#   0.03991391 0.03638681 0.07585304 0.02402204 0.04653621 0.04003545
#   0.09107496 0.03884495 0.04577003 0.05613577 0.06198267 0.03114659
#   0.03537885 0.04659643 0.02441043 0.03536744 0.03815489 0.02189533
#   0.02085931 0.03553282 0.0045371  0.04765456 0.01046097 0.02031535
#   0.05407663 0.03156354 0.0386354  0.04832357 0.09058022 0.04235563
#   0.04016078 0.02497608 0.03865574 0.05150859 0.02163364 0.06337045
#   0.02308722 0.02846877 0.009777   0.02550285 0.01909062 0.04070752
#   0.0412812  0.0296858  0.07952902 0.06188233 0.04839746 0.05954292
#   0.05387438 0.05117965 0.03328561 0.03647665 0.03985072 0.03393035
#   0.04250537 0.02962263 0.08436558 0.05645901 0.01886168 0.0308288
#   0.0311693  0.05461103 0.07239735 0.02228967 0.03618345 0.02673859
#   0.04808396 0.05569021 0.03209361 0.02183886 0.05405376 0.02347033
#   0.03127245 0.03917034 0.04544007 0.09066812 0.05626544 0.03999475
#   0.04076502 0.0413091  0.06966481 0.03670895 0.01445661 0.03503531
#   0.04241377 0.09374386 0.03384844 0.04887306 0.01335535 0.02995017
#   0.06865145 0.03352662 0.01827323 0.0507638  0.03019871 0.03167158
#   0.03357492 0.03229717 0.01440746 0.04255149 0.05157927 0.03900907
#   0.03503184 0.03066203 0.03176732 0.01316799 0.07340474 0.04265228
#   0.01729919 0.026262   0.02747191 0.07747789 0.00885222 0.07009781
#   0.06450921 0.07533725 0.0276358  0.03016797 0.01159841 0.0173345
#   0.01974459 0.0327532  0.04294292 0.06131923 0.03160696 0.03153466
#   0.03055558 0.09338229 0.05683999 0.02289821 0.05917569 0.03800795
#   0.02693194 0.05059473 0.02500182 0.06030395 0.02978107 0.04882079
#   0.09238016 0.03612835 0.0165355  0.01516278 0.04044069 0.05358681
#   0.03305457 0.07482649 0.06918632 0.01888238 0.02331081 0.04951568
#   0.06082922 0.01642808 0.05289252 0.03168466 0.02604803 0.03935675
#   0.05449121 0.03986618 0.04544565 0.0471457  0.03630655 0.03297597
#   0.02128312 0.07635617 0.03322757 0.0326846  0.02375737 0.01902725
#   0.0386374  0.06982048 0.08305487 0.03022983 0.02265134 0.03337473
#   0.04840991 0.06190139 0.0638769  0.03547415 0.03177813 0.04696455
#   0.01011204 0.04374461 0.023197   0.03254153 0.03899942 0.02901819
#   0.03964443 0.02570702 0.02505174 0.06089385 0.02443788 0.02659461
#   0.05842128 0.02381614 0.0411226  0.03862537 0.02365448 0.01713652
#   0.02148333 0.03380116 0.03660134 0.04479866 0.04238143 0.0478809
#   0.02690394 0.04504631 0.03366153 0.04110901 0.07192478 0.0310457
#   0.05477953 0.05149761 0.02893589 0.05948433 0.04935074 0.01427747
#   0.03347059 0.04828879 0.03123127 0.02066584 0.06135492 0.00299935
#   0.03048544 0.02589185 0.03200752 0.0090923  0.01842138 0.04486928
#   0.02024946 0.04962607 0.0404489  0.03843744 0.03526874 0.03518821
#   0.02978507 0.05356523 0.03503129 0.05415602 0.0424846  0.03343526
#   0.08354013 0.06306652 0.02539925 0.05152532 0.04132864 0.06045302
#   0.0395395  0.01947541 0.07636737 0.0618382  0.04125489 0.02367874
#   0.02489864 0.0121386  0.04563442 0.0315078  0.05973774 0.02774201
#   0.04604561 0.04852458 0.04041621 0.04666892 0.02624613 0.01342819
#   0.03670382 0.00752382 0.05573861 0.04705926 0.02297265 0.03988333
#   0.01884146 0.03258308 0.03491301 0.02805855 0.02130414 0.05364816
#   0.0220966  0.03565615 0.07019095 0.04768172 0.02415338 0.0199276
#   0.02697019 0.0575205  0.023189   0.05944121 0.07309026 0.05971579
#   0.02257736 0.04635005 0.03133521 0.04347963 0.06893823 0.04763685
#   0.06739016 0.02544807 0.03323688 0.02129794 0.02760325 0.05694502
#   0.05636752 0.03660777 0.04881699 0.04237913 0.05617797 0.02622373
#   0.03403307 0.06314882 0.05370798 0.04813522 0.01567575 0.0526628
#   0.03355744 0.08996839 0.06914256 0.07079489 0.05027865 0.02630138
#   0.05996167 0.02145928 0.01774483 0.01420775 0.04193863 0.07878247
#   0.07227221 0.05259415 0.03842139 0.04425631 0.03356641 0.06896188
#   0.04600126 0.06330042 0.06466863 0.04129824 0.04614133 0.01843603
#   0.04164365 0.05102567 0.07700988 0.04224495 0.04057857 0.04512722
#   0.04733758 0.05751148 0.07274308 0.05289296 0.0387125  0.04306265
#   0.07220802 0.0557955  0.07724866 0.00726269 0.05875438 0.0339262
#   0.04806245 0.0250742  0.07917519 0.02464008 0.04484735 0.0447369
#   0.01740213 0.02952509 0.04999662 0.04943816 0.04355759 0.07831026
#   0.05956578 0.03426943 0.0267984  0.03965076 0.01469689 0.02579744
#   0.07447392 0.01895773 0.05281512 0.01800102 0.08016083 0.04742664
#   0.02630896 0.03448641 0.05531719 0.02024027 0.02216274 0.01758332
#   0.01744621 0.03972851 0.0291498  0.02787461 0.0626196  0.07268924
#   0.02101126 0.05211816 0.0647471  0.03578633 0.0252502  0.05702175
#   0.03987198 0.02933541 0.04475092 0.04715088 0.04860883 0.05116592
#   0.0485709  0.03762124 0.04732252 0.0370212  0.04430133 0.02809027
#   0.01966434 0.05158924 0.02997978 0.04364362 0.01902841 0.04168759
#   0.02929617 0.03092925 0.04429359 0.02837896 0.02080145 0.05305761
#   0.02591461 0.04838872 0.04871628 0.02158341 0.04903964 0.02510862
#   0.03950535 0.04584167 0.03920467 0.01867216 0.05002987 0.039427
#   0.05036042 0.03265887 0.04687266 0.06406225 0.03522342 0.0432227
#   0.00803539 0.04532195 0.03730162 0.02342325 0.02983546 0.040707
#   0.02820702 0.03244109 0.0250306  0.04442893 0.03705835 0.06770574
#   0.05419899 0.05450385 0.03231128 0.00989086 0.05440119 0.01967065
#   0.06473447 0.03675486 0.02435249 0.0840313  0.01263148 0.04379898
#   0.02139808 0.06513631 0.02373353 0.0306862  0.06847857 0.03757589
#   0.03037347 0.02063059 0.0356409  0.00285662 0.06703972 0.03967095
#   0.02041161 0.04025241 0.05076327 0.02714908 0.02348886 0.03010125
#   0.0387229  0.05050784 0.03605653 0.04057087 0.03854706 0.04362792
#   0.0445853  0.03711591 0.02701274 0.03696327 0.04410639 0.00408744
#   0.01943306 0.05524526]]